<html>
<head><meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
	
	<title>Paper Reading Seminar</title>
    <style type="text/css">
    body {
        font-family: cambria, calibri, garamond, century, gulim, dotum, arial;
        font-size: 18px;
        margin: 0 auto;
        width: 90%;
        min-width: 200px;
        max-width: 1200px;
        width: expression_r(Math.max(200, Math.min(1200, document.body.offsetWidth-40)) + "px");
    }
	</style>
	<style type="text/css">
	</style>

<style>
table {
    font-family: cambria, calibri, garamond, century, gulim, dotum, arial;
    border-collapse: collapse;
    width: 100%;
    font-size:18px;
}

td, th {
    border: 1px solid black;
    text-align: left;
    padding: 8px;
}

tr:nth-child(even) { background-color: #F6F2F2; }

</style>

</head>
<body>


<p></p>
<p></p>


<b>Organizer</b>: <a href="http://zptu.net">Zhaopeng Tu</a>
<br>
<b>Conferences</b>: ACL, EMNLP, NIPS, ICML, ICLR, NAACL, etc
<br>
<b>Focuses</b>: NLP (MT, QA, Dialogue, etc), DL (RL, GAN, VAE, etc)
<br>
<b>Guideline</b>: A good presentation should consist of the following parts: motivation, approach, experiments, pro & con of this work, inspiration from this work. Each part can be presented with 2~3 slides, and the last two parts are most important thus should be well-prepared.
<br>

<p></p>

<br><br>

<p></p>

<table>
<tr>
  <th>Date</th> 
  <th>Title</th> 
  <th>Presenter</th> 
</tr>


<tr> 
  <td rowspan=1>2020-08-20 <br><br> (3:30pm~5:30pm, Room 4209)</td>
  <td>
    Rethinking the <i>Pretraining Models</i> for NMT [<a href="slides/20200820_jwxwang.pdf"><font color="#0080FF">slides</font></a >]<br>
    <a href="https://arxiv.org/pdf/2003.08271.pdf">Pre-trained Models for Natural Language Processing: A Survey</a > (Invited Review 2020)<br>
    <a href="https://arxiv.org/pdf/2002.06823.pdf">Incorporating BERT into Neural Machine Translation</a > (ICLR 2020)<br>
    <a href="https://arxiv.org/abs/2001.08210">Multilingual Denoising Pre-training for Neural Machine Translation</a > (Arxiv 2020)<br>
    <a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/He_Rethinking_ImageNet_Pre-Training_ICCV_2019_paper.pdf">Rethinking ImageNet Pre-Training</a > (ICCV 2019)<br>
    <a href="https://arxiv.org/abs/2006.06882">Rethinking Pre-training and Self-training</a > (Arxiv 2020)<br>
    <a href="https://www.aclweb.org/anthology/2020.acl-main.200.pdf">To Pretrain or Not to Pretrain: Examining the Benefits of Pretrainng on Resource Rich Tasks </a > (ACL 2020)<br>
    <a href="https://www.aclweb.org/anthology/2020.acl-main.467.pdf">Intermediate-Task Transfer Learning with Pretrained Language Models: When and Why Does It Work?</a > (ACL 2020)<br>
    <a href="https://www.aclweb.org/anthology/2020.acl-main.740.pdf">Don't Stop Pretraining: Adapt Language Models to Domains and Tasks</a > (ACL 2020)<br>
    <a href="https://www.aclweb.org/anthology/P19-1441.pdf">Multi-Task Deep Neural Networks for Natural Language Understanding</a > (ACL 2019)<br>
    <a href="https://www.aclweb.org/anthology/2020.acl-main.200.pdf">To Pretrain or Not to Pretrain: Examining the Benefits of Pretrainng on Resource Rich Tasks </a > (ACL 2020)<br>
    <a href="https://arxiv.org/abs/2005.00944">Understanding and Improving Information Transfer in Multi-Task Learning</a > (ICLR 2020)<br>
    <a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Kornblith_Do_Better_ImageNet_Models_Transfer_Better_CVPR_2019_paper.pdf">Do Better ImageNet Models Transfer Better?</a > (CVPR 2019)<br>
    <a href="https://openreview.net/pdf?id=B1g8VkHFPH">Rethinking the Hyperparameters for Fine-tuning</a > (ICLR 2020)<br>
    <a href="https://www.aclweb.org/anthology/2020.acl-main.688/">In Neural Machine Translation, What Does Transfer Learning Transfer? </a > (ACL 2020)<br>
    <a href="https://arxiv.org/abs/1909.12031">Towards Understanding the Transferability of Deep Representations</a > (Arxiv 2019)<br>
    <a href="https://arxiv.org/abs/2004.06100">Pretrained Transformers Improve Out-of-Distribution Robustness </a > (ACL 2020)<br>
  </td>
  <td>  
    <a href="https://scholar.google.com/citations?user=4v5x0bUAAAAJ&hl=en">Wenxuan Wang</a > <br>
  </td>
</tr>


<tr> 
  <td rowspan=1>2020-08-06 <br><br> (3:30pm~5:30pm, Room 3104)</td>
  <td>
    On the <i>Learning of Patterns</i> in Deep Neural Networks [<a href="slides/20200806_wxjiao.pdf"><font color="#0080FF">slides</font></a >]<br>
    <a href="https://icml.cc/Conferences/2017/ScheduleMultitrack?event=1327">A Closer Look at Memorization in Deep Networks</a > (ICML 2017)<br>
    <a href="https://openreview.net/forum?id=HkxHv4rn24">Do deep neural networks learn shallow learnable examples first?</a > (ICML 2019 Workshop)<br>
    
    <br><br>
    On recent approaches for <i>Subword Regularization</i> [<a href="slides/20200806_ychao.pdf"><font color="#0080FF">slides</font></a >]<br>
    <a href="https://www.aclweb.org/anthology/P18-1007/">Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates</a > (ACL 2018)<br>
    <a href="https://arxiv.org/abs/2004.14109">Adversarial Subword Regularization for Robust Neural Machine Translation</a > (arxiv 2020)<br>
    <a href="https://www.aclweb.org/anthology/2020.acl-main.170/">BPE-Dropout: Simple and Effective Subword Regularization</a > (ACL 2020)<br>
  </td>
  <td>  
    <a href="https://wxjiao.github.io">Wenxiang Jiao</a > <br>
    <a href="https://github.com/yongchanghao">Yongchang Hao</a > <br>
  </td>
</tr>

<tr> 
  <td rowspan=1>2020-07-30 <br><br> (3:30pm~5:30pm, Room 4310)</td>
  <td>
    On the <i>Jacobian Matrix</i> of Neural Networks [<a href="slides/20200730_cxdu.pdf"><font color="#0080FF">slides</font></a >]<br>
    <a href="https://openreview.net/forum?id=HJC2SzZCW">Sensitivity and Generalization in Neural Networks: an Empirical Study</a> (ICLR 2018)<br>
    <a href="https://arxiv.org/abs/1908.02729">Robust Learning with Jacobian Regularization</a> (arXiv 2019)<br>
    <a href="https://openreview.net/forum?id=Hke0V1rKPS">Jacobian Adversarially Regularized Networks for Robustness</a> (ICLR 2020)<br>
    
    <br><br>
    Extensive Analyses on <i>Beam Search for Sequence Generation</i> [<a href="slides/20200730_swang.pdf"><font color="#0080FF">slides</font></a >]<br>
    <a href="https://www.aclweb.org/anthology/D17-1227/">When to Finish? Optimal Beam Search for Neural Text Generation</a> (EMNLP 2017)<br>
    <a href="https://www.aclweb.org/anthology/W18-6322/">Correcting Length Bias in Neural Machine Translation</a> (WMT 2018)<br>
    <a href="https://www.aclweb.org/anthology/D19-1331/">On NMT Search Errors and Model Errors: Cat Got Your Tongue?</a> (EMNLP 2019)<br>
    <a href="http://proceedings.mlr.press/v97/cohen19a.html">Empirical Analysis of Beam Search Performance Degradation in Neural Sequence Models</a> (ICML 2019)<br>
    <a href="https://www.aclweb.org/anthology/2020.acl-main.326/">On Exposure Bias, Hallucination and Domain Shift in Neural Machine Translation</a> (ACL 2020)<br>
  </td>
  <td>  
    <a href="https://nonvolatilememory.github.io">Cunxiao Du</a> <br>
    <a href="https://shuowang.cool">Shuo Wang</a> <br>
  </td>
</tr>

<tr> 
  <td rowspan=1>2020-07-23 <br><br> (4:00pm~5:00pm, Room 2801)</td>
  <td>
    Towards Better NAT Encoder & Decoder [<a href="slides/20200723_lding.pdf"><font color="#0080FF">slides</font></a >]
  </td>
  <td>
    Liang Ding
  </td>
</tr>

<tr> 
  <td rowspan=1>2020-07-09 <br><br> (4:00pm~5:30pm, Room 3905)</td>
  <td>
    <a href="https://papers.nips.cc/paper/9341-towards-explaining-the-regularization-effect-of-initial-large-learning-rate-in-training-neural-networks.pdf">Towards Explaining the Regularization Effect of Initial Large Learning Rate in Training Neural Networks</a> (NeurIPS 2019) [<a href="slides/20200709_wxjiao.pdf"><font color="#0080FF">slides</font></a >]<br>
    
    Survey on <i>Low-Complexity Attentions for Transformer</i> [<a href="slides/20200709_ychao.pdf"><font color="#0080FF">slides</font></a >]
  </td>
  <td>  
    <a href="https://wxjiao.github.io">Wenxiang Jiao</a> <br>
    Yongchang Hao
  </td>
</tr>

<tr> 
  <td rowspan=1>2020-07-02 <br><br> (4:00pm~5:30pm, Room 3905)</td>
  <td>
    <a href="https://arxiv.org/abs/2006.10288">Individual Calibration with Randomized Forecasting</a> (ICML 2020) [<a href="slides/20200702_swang.pdf"><font color="#0080FF">slides</font></a >]<br>
    
    <a href="http://www.iro.umontreal.ca/~memisevr/pubs/AEenergy.pdf">The Potential Energy of an Autoencoder</a> (TPAMI, 2014) [<a href="slides/20200702_cxdu.pdf"><font color="#0080FF">slides</font></a >]<br>
    
    <a href="https://www.aclweb.org/anthology/D17-1209.pdf">Graph Convolutional Encoders for Syntax-aware Neural Machine Translation</a> (EMNLP 2017) [<a href="slides/20200702_xchuang.pdf"><font color="#0080FF">slides</font></a >]
  </td>
  <td>  
    <a href="https://shuowang.cool">Shuo Wang</a> <br>
    <a href="https://nonvolatilememory.github.io">Cunxiao Du</a> <br>
    <a href="https://scholar.google.com/citations?user=UvRiNH4AAAAJ">Xuancheng Huang</a>
  </td>
</tr>



<tr>
  <td rowspan=1>2019-12-31 <br><br> (7:00pm~10:00pm, Room 4202)</td>
  <td>
    Survery on <i>Non-Autoregressive NMT and Knowledge Distallation</i> [<a href="slides/20191231_lywang_xbliu_lding.pdf"><font color="#0080FF">slides</font></a >]
    <br><br>
    <i>Non-Autoregressive NMT</i><br>
    <a href="https://arxiv.org/abs/1904.09324">Mask-Predict: Parallel Decoding of Conditional Masked Language Models</a > (EMNLP 2019)<br>
    <a href="https://arxiv.org/abs/1905.11006">Levenshtein Transformer</a > (NeurIPS 2019)<br>
    <br>

    <i>Conditional Generation from Knowledge Distillation Perspective</i><br>
    <a href="https://arxiv.org/pdf/1503.02531.pdf">Distilling the Knowledge in a Neural Network</a > (NeurIPS 2014 Workshop)<br>
    <a href="https://www.aclweb.org/anthology/D16-1139.pdf">Sequence-Level Knowledge Distillation</a > (EMNLP 2016)<br>
    <a href="https://openreview.net/pdf?id=BygFVAEKDH">Understanding Knowledge Distillation in Non-Autoregressive MT</a > (ICLR 2020)<br>
    <a href="https://openreview.net/pdf?id=S1efxTVYDr">Data-Dependent Gaussian Prior Objective for Language Generation</a > (ICLR 2020)<br>
  </td>
  <td>
    <a href="http://www.longyuewang.com/">Longyue Wang</a > <br>
    Xuebo Liu <br>
    Liang Ding
  </td>
</tr>

<tr> 
  <td rowspan=1>2019-12-25 <br><br> (3:30pm~6:00pm, Room 4310)</td>
  <td>
    <b>On the Information in Deep Networks I</b>: <i>Information Bottleneck</i> [<a href="slides/20191225_zptu_wxwang_bywang_cxdu.pdf"><font color="#0080FF">slides</font></a>]
    <br><br>
    <i>Information Bottleneck (IB) Theory of Deep Learning</i><br>
    <a href="https://arxiv.org/abs/1503.02406">Deep Learning and the Information Bottleneck Principle</a> (IEEE ITW 2015)<br>
    <a href="https://arxiv.org/abs/1703.00810">Opening the Black Box of Deep Neural Networks via Information</a> (arXiv 2017)<br>
    <a href="https://openreview.net/forum?id=ry_WPG-A-">On the Information Bottleneck Theory of Deep Learning</a> (ICLR 2018)<br>
    <br>
    
    <i>Information Flow: IB for Attribution</i><br>
    <a href="proceedings.mlr.press/v97/guan19a/guan19a.pdf">Estimating Information Flow in Deep Neural Networks</a> (ICML 2019)<br>
    <a href="https://www.aclweb.org/anthology/D19-1276.pdf">Towards a Deep and Unified Understanding of Deep Neural Models in NLP</a> (ICML 2019)<br>
    <a href="https://openreview.net/forum?id=S1xWh1rYwB">Restricting the Flow: Information Bottlenecks for Attribution</a> (ICLR 2020)<br>
    <br>
    
    <i>Variational Information Bottleneck (VIB) and its Application</i><br>
    <a href="https://arxiv.org/abs/1612.00410">Deep Variational Information Bottleneck</a> (ICLR 2017)<br>
    <a href="https://www.aclweb.org/anthology/D19-1276.pdf">Specializing Word Embeddings (for Parsing) by Information Bottleneck</a> (EMNLP 2019, Best Paper)<br>
  </td>
  <td>  
    <a href="http://zptu.net">Zhaopeng Tu</a> <br>
    Boyuan Wang <br>
    <a href="https://nonvolatilememory.github.io">Cunxiao Du</a> <br>
    Wenxuan Wang
  </td>
</tr>

<tr> 
  <td rowspan=1>2019-12-19 <br><br> (2pm~5pm, Room 3604)</td>
  <td>
    Survery on <i>Exploiting Monolingual Data for NMT</i> [<a href="slides/20191219_xwang_wxjiao_slhe.pdf"><font color="#0080FF">slides</font></a>]
    <br><br>
    <a href="https://www.computing.dcu.ie/~gmdbwenniger/InvestigatingBacktranslationInNMT.pdf">Investigating Backtranslation in Neural Machine Translation</a> (EAMT 2018) <br>
    
    <a href="https://www.aclweb.org/anthology/D18-1045.pdf">Understanding Back-Translation at Scale</a> (EMNLP 2018) <br>
    <a href="https://www.aclweb.org/anthology/D18-1040.pdf">Back-Translation Sampling by Targeting Difficult Words in Neural Machine Translation</a> (EMNLP2018) <br>
    
    <a href="http://www.statmt.org/wmt19/pdf/52/WMT06.pdf">Tagged Back-Translation</a> (WMT 2019) <br>
    <a href="https://www.microsoft.com/en-us/research/uploads/prod/2019/11/Exploiting-Monolingual-Data-at-Scale-for-Neural-Machine-Translation.pdf">Exploiting Monolingual Data at Scale for Neural Machine Translation</a> (EMNLP 2019) <br>
    <a href="https://arxiv.org/pdf/1911.01986.pdf">Data Diversification: An Elegant Strategy For Neural Machine Translation</a> (arXiv 2019) <br>

  </td>
  <td>  
    <a href="http://www.xingwang4nlp.com">Xing Wang</a> <br>
    <a href="https://shilinhe.github.io">Shilin He</a> <br>
    <a href="https://wxjiao.github.io">Wenxiang Jiao</a>
  </td>
</tr>

<tr> 
  <td rowspan=1>2019-10-29 <br><br> (7pm~9pm, Room 811)</td>
  <td>
    Tips on Writing a Research Paper
  </td>
  <td> <a href="http://zptu.net">Zhaopeng Tu</a> </td>
</tr>

<tr> 
  <td rowspan=1>2019-07-10 -- 2019-12-10 </td>
  <td>
    Discussions on research projects for ACL2020
  </td>
  <td> All Members </td>
</tr>

<tr> 
  <td rowspan=1>2019-07-03 <br><br> (2pm~5pm, Room 809)</td>
  <td>
    Survery on <i>Analysis of Transformer</i>, covering  [<a href="slides/20190703_wxwang.pdf"><font color="#0080FF">slides</font></a>]
    <br><br>
    How Much Attention Do You Need? A Granular Analysis of Neural Machine Translation Architectures (ACL 2018) <br>
    What Does BERT Look At? An Analysis of BERT's Attention(ACL 2019 Workshop) <br>
    Is Attention Interpretable?(ACL 2019) <br>
    Analyzing the Structure of Attention in a Transformer Language Model (ACL 2019 Workshop) <br>
    An Analysis of Encoder Representations in Transformer-Based Machine Translation(EMNLP 2018 Workshop) <br>
    Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned(ACL 2019) <br>
    Are Sixteen Heads Really Better than One? (Submitted to NeurIPS2019) <br>
  </td>
  <td> Wenxuan Wang </td>
</tr>

<tr> 
  <td rowspan=1>2019-06-26 <br><br> (2pm~5pm, Room 809)</td>
  <td>
    Survery on <i>Detect and Understand Generalization Barriers</i>, covering  [<a href="slides/20190626_glli.pdf"><font color="#0080FF">slides</font></a>]
    <br><br>
    <a href="https://arxiv.org/abs/1610.02136">A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks</a> (ICLR 2017) <br>
    <a href="https://arxiv.org/abs/1807.03888">A Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks</a> (NeurIPS 2018) <br>
    <a href="https://arxiv.org/abs/1810.09136">Do Deep Generative Models Know What They Don't Know?</a> (ICLR 2019) <br>
    <a href="https://arxiv.org/abs/1805.04604">Confidence Modeling for Neural Semantic Parsing</a> (EMNLP 2018) <br>
    <a href="https://www.aclweb.org/anthology/D18-1036">Addressing Troublesome Words in Neural Machine Translation</a> (EMNLP 2018) <br>
    <a href="https://arxiv.org/abs/1703.04730">Understanding Black-box Predictions via Influence Functions</a> (ICML 2017) <br>
    <a href="https://arxiv.org/abs/1904.02868">Data Shapley: Equitable Valuation of Data for Machine Learning</a> (ICML 2019) <br>
    <a href="https://arxiv.org/abs/1905.13289">On the Accuracy of Influence Functions for Measuring Group Effects</a> (NeurIPS 2019 submitted) <br>
  </td>
  <td> <a href="https://github.com/Epsilon-Lee">Guanlin Li</a> </td>
</tr>


<tr> 
  <td rowspan=1>2019-06-19 <br><br> (2pm~5pm, Room 1206)</td>
  <td>
    Survery on <i>Identifying and Controlling Individual Neurons</i>, covering  [<a href="slides/20190619_jhao.pdf"><font color="#0080FF">slides</font></a>]
    <br><br>
    <a href="https://openreview.net/forum?id=H1z-PsR5KX">Identifying and Controlling Important Neurons in Neural Machine Translation</a> (ICLR 2019) <br>
    <a href="https://openreview.net/forum?id=SylKoo0cKm">How Important is a Neuron</a> (ICLR 2019) <br>
    <a href="https://arxiv.org/abs/1812.09355">What Is One Grain of Sand in the Desert? Analyzing Individual Neurons in Deep NLP Models</a> (AAAI 2019) <br>
    <a href="https://arxiv.org/abs/1812.00481">Neural Rejuvenation: Improving Deep Network Training by Enhancing Computational Resource Utilization</a> (CVPR 2019)
  </td>
  <td> <a href="https://demon-jiehao.github.io/">Jie Hao</a> </td>
</tr>

<tr> 
  <td rowspan=2>2019-06-12 <br><br> (2:30pm~5:00pm, Room 811)</td>
  <td>
    Survery on <i>On the Diversity of Beam Search</i>, covering  [<a href="slides/20190612_ylyang.pdf"><font color="#0080FF">slides</font></a>]
    <br><br>
    Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence Models (AAAI 2018) <br>
    Analyzing Uncertainty in Neural Machine Translation (ICML 2018) <br>
    Mixture Models for Diverse Machine Translation: Tricks of the Trade (ICML 2019)
  </td>
  <td> <a href="http://yilinyang7.github.io">Yilin Yang</a> </td>
</tr>  
<tr>
  <td>
    Survery on <i>Learning to Reweight Examples for Denoising</i>, covering  [<a href="slides/20190612_slhe.pdf"><font color="#0080FF">slides</font></a>]
    <br><br>
    Learning to Reweight Examples for Robust Deep Learning (ICML 2018) <br>
    MentorNet: Learning Data-Driven Curriculum for Very Deep Neural Networks on Corrupted Labels (ICML 2018)
  </td>
  <td> <a href="https://shilinhe.github.io">Shilin He</a> </td>
</tr>

<tr> 
  <td rowspan=1>2019-06-05 <br><br> (2:30pm~5:00pm, Room 811)</td>
  <td>
    Survery on <i>Transfer Learning for Neural Machine Translation</i>, covering  [<a href="slides/20190605_ywang.pdf"><font color="#0080FF">slides</font></a>]
    <br><br>
    A Survey of Multilingual Neural Machine Translation (arXiv 2019)<br>
    Massively Multilingual Neural Machine Translation (NAACL 2019) <br> 
    Multilingual Neural Machine Translation With Soft Decoupled Encoding (ICLR 2019) <br>             
    Effective Cross-lingual Transfer of Neural Machine Translation Models without Shared Vocabularies (ACL 2019)<br>

    Non-Parametric Adaptation for Neural Machine Translation (NAACL 2019) <br>
    Curriculum Learning for Domain Adaptation in Neural Machine Translation (NAACL 2019) <br> 
    Domain Adaptation of Neural Machine Translation by Lexicon Induction (ACL 2019) <br>
    Domain Adaptive Inference for Neural Machine Translation (ACL 2019) <br>
  </td>
  <td> Yong Wang </td>
</tr>


<tr> 
  <td rowspan=1>2019-05-29 <br><br> (7:00pm~9:00pm, Room 809)</td>
  <td>
    Survery on <i>Unsupervised Neural Machine Translation</i>, covering  [<a href="slides/20190529_hyli.pdf"><font color="#0080FF">slides</font></a>]
    <br><br>
    Dual Learning for Machine Translation (NIPS2016) <br>
    Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks (ICCV2017) <br>
    Style Transfer from Non-Parallel Text by Cross-Alignment (NIPS2017) <br>
    Phrase-Based & Neural Unsupervised Machine Translation (EMNLP2018) <br>
    Unsupervised Neural Machine Translation with Weight Sharing (ACL2018) <br>
    Unsupervised Machine Translation Using Monolingual Corpora only (ICLR2018) <br>
    Unsupervised Neural Machine Translation (ICLR2018) <br>
    Extract and Edit: An Alternative to Back-Translation for Unsupervised Neural Machine Translation (NAACL2019) <br>
  </td>
  <td> Huayang Li </td>
</tr>

<tr> 
  <td rowspan=1>2019-04-17 <br><br> (Room 809)</td>
  <td>
    Survery on <i>Curriculum Learning -- From Easy to Hard, Learning Order Matters!</i>, covering  [<a href="slides/20190417_slhe.pdf"><font color="#0080FF">slides</font></a>]
    <br><br>
    Competence-based Curriculum Learning for Neural Machine Translation (NAACL 2019) <br>
    Denoising Neural Machine Translation Training with Trusted Data and Online Data Selection (WMT 2018) <br>
  </td>
  <td> <a href="https://shilinhe.github.io">Shilin He</a> </td>
</tr>

<tr> 
  <td rowspan=1>2019-04-10 <br><br> (Room 809)</td>
  <td>
    Survery on <i>Optimizing Beam Search for Seq2Seq Learning</i>, covering [<a href="slides/20190410_ylyang.pdf"><font color="#0080FF">slides</font></a>]
    <br><br>
    Breaking the Beam Search Curse: A Study of (Re-)Scoring Methods and Stopping Criteria for NMT (EMNLP 2018) <br>
    Learning to Stop in Structured Prediction for Neural Machine Translation (NAACL 2019) <br>
    Modeling Past and Future for Neural Machine Translation (TACL 2018) <br>
    An Actor-Critic Algorithm for Sequence Prediction (ICLR 2017) <br>
    Learning to Decode for Future Success (arXiv 2017) <br>
    Decoding with Value Networks for Neural Machine Translation (NIPS 2017) <br>
    Learning to Act by Predicting the Future (ICLR 2017)
  </td>
  <td> <a href="http://yilinyang7.github.io">Yilin Yang</a> </td>
</tr>

<tr> 
  <td rowspan=1>2019-04-03 <br><br> (Room 811)</td>
  <td>
    How to Take <i>Reusable</i> Paper Notes?
    <br>&<br>
    How to Draw <i>Acceptable</i> Figures? 
  </td>
  <td> <a href="http://zptu.net">Zhaopeng Tu</a> </td>
</tr>

<tr>
  <td rowspan=1>2019-03-27 <br><br> (Room 809)</td>
  <td>
    <a href="http://nlp.csai.tsinghua.edu.cn/~ly/talks/cwmt14_tut.pdf">How to Write a Research Paper? (Yang Liu)</a>
  </td>
  <td> <a href="https://demon-jiehao.github.io/">Jie Hao</a>, <a href="http://ir.hit.edu.cn/~xwgeng/">Xinwei Geng</a>, and Yong Wang </td>
</tr>

<tr> 
  <td rowspan=1>2019-03-20 <br><br> (Room 810)</td>
  <td>
    Survery on <i>Learning Decay</i>, covering [<a href="slides/20190320_jhao.pdf"><font color="#0080FF">slides</font></a>]
    <br><br>
    A Stochastic Approximation Method (The Annals of Mathematical Statistics 1951) <br>
    Don't Decay the Learning Rate, Increase the Batch Size (ICLR 2019) <br>
    A Bayesian Perspective on Generalization and Stochastic Gradient Descent (ICLR 2018) <br>
    Adam: A Method for Stochastic Optimization (ICLR 2015)
  <td> <a href="https://demon-jiehao.github.io/">Jie Hao</a> </td>
</tr>

<tr> 
  <td rowspan=1>2019-03-13 <br><br> (Room 810)</td>
  <td>
    Survery on <i>Domain Adaptation</i>, covering [<a href="slides/20190313_ywang.pdf"><font color="#0080FF">slides</font></a>]
    <br><br>
    A Survey of Domain Adaptation for Neural Machine Translation (COLING 2018) <br>
    Multi-Domain Neural Machine Translation with Word-Level Domain Context Discrimination (EMNLP2018) <br>
    Contextual Parameter Generation for Universal Neural Machine Translation (EMNLP 2018) <br>
    BERT and PALs- Projected Attention Layers for Efficient Adaptation in Multi-Task Learning (arXiv 2019)
  <td> Yong Wang </td>
</tr>

<tr> 
  <td rowspan=3>2019-01-03</td>
  <td>
    The Information Bottleneck Method (AAC-CCC, 1999) [<a href="slides/20190103_bsyang.pdf"><font color="#0080FF">slides</font></a>] <br/>
    Deep Learning and the Information Bottleneck Principle (Information Theory Workshop, 2015)
  <td> <a href="https://scholar.google.com/citations?hl=zh-CNBaosong Yanguser=fXsHJXkAAAAJ">Baosong Yang</a> </td>
</tr>
<tr>
  <td>
    Opening the Black Box of Deep Neural Networks via Information (arXiv 2017) [<a href="slides/20190103_slhe.pdf"><font color="#0080FF">slides</font></a>] 
  </td> 
  <td> <a href="https://shilinhe.github.io">Shilin He</a> </td>
</tr>
<tr>
  <td>
    <a href="https://openreview.net/forum?id=ry_WPG-A-&noteId=ry_WPG-A-">On the Information Bottleneck Theory of Deep Learning</a> (ICLR 2018) [<a href="slides/20190103_jli.pdf"><font color="#0080FF">slides</font></a>] 
  </td>
  <td> <a href="http://appsrv.cse.cuhk.edu.hk/~jianli/">Jian Li</a> </td>
</tr>

<tr> 
  <td rowspan=3>2018-12-25</td>
  <td>
    <i>Recent Progresses on Document-level NMT</i>, covering [<a href="slides/20181225_lywang.pdf"><font color="#0080FF">slides</font></a>] <br/><br/>
    Document-level Neural Machine Translation with Hierarchical Attention Networks (EMNLP 2018) <br/>
    Improving the Transformer Translation Model with Document-level Context (EMNLP 2018) <br/>
    Modelling Coherence for Discourse Neural Machine Translation (AAAI 2019)
  </td>
  <td> <a href="http://www.longyuewang.com">Longyue Wang</a> </td>
</tr>
<tr>
  <td>
    <a href="https://openreview.net/forum?id=Hyg_X2C5FX">Visualizing and Understanding Generative Adversarial Networks</a> (ICLR 2019) [<a href="slides/20181225_xwgeng.pdf"><font color="#0080FF">slides</font></a>]
  </td> 
  <td> <a href="http://ir.hit.edu.cn/~xwgeng/">Xinwei Geng</a> </td>
</tr>
<tr>
  <td>
    <a href="https://openreview.net/forum?id=ByxZX20qFQ">Adaptive Input Representations for Neural Language Modeling</a> (ICLR 2019) [<a href="slides/20181225_jli.pdf"><font color="#0080FF">slides</font></a>]
  </td>
  <td> <a href="http://appsrv.cse.cuhk.edu.hk/~jianli/">Jian Li</a> </td>
</tr>

<tr> 
  <td rowspan=3>2018-11-27</td>
  <td>
    <i>Robustness in NLP: A Short Survey</i>, covering [<a href="slides/20181127_slhe.pdf"><font color="#0080FF">slides</font></a>]<br/><br/>
    <a href="https://openreview.net/forum?id=BJ8vJebC-">Synthetic and Natural Noise Both Break Neural Machine Translation</a> (ICLR 2018) <br/>
    Robust Neural Machine Translation with Joint Textual and Phonetic Embedding (arXiv 2018) <br/>
    Improving the Robustness of Speech Translation (arXiv 2018)
  </td>
  <td> <a href="https://shilinhe.github.io">Shilin He</a> </td>
</tr>
<tr>
  <td>
    <a href="https://openreview.net/forum?id=BkgPajAcY7">No Training Required: Exploring Random Encoders for Sentence Classification</a> (ICLR 2019) [<a href="slides/20181127_jhao.pdf"><font color="#0080FF">slides</font></a>]
  </td> 
  <td> <a href="https://demon-jiehao.github.io/">Jie Hao</a> </td>
</tr>
<tr>
  <td>
    <a href="https://openreview.net/forum?id=SkVhlh09tX">Pay Less Attention with Lightweight and Dynamic Convolutions</a> (ICLR 2019) [<a href="slides/20181127_bsyang.pdf"><font color="#0080FF">slides</font></a>]
  </td>
  <td> <a href="https://scholar.google.com/citations?user=fXsHJXkAAAAJ">Baosong Yang</a> </td>
</tr>

<tr> 
  <td rowspan=4>2018-11-20</td>
  <td>
    Multi-Labeled Relation Extraction with Attentive Capsule Network (AAAI 2019) [<a href="slides/20181120_jli.pdf"><font color="#0080FF">slides</font></a>]
  </td> 
  <td> <a href="http://appsrv.cse.cuhk.edu.hk/~jianli/">Jian Li</a> </td>
</tr>
<tr>
  <td>
     <a href="https://openreview.net/forum?id=B1l6qiR5F7">Ordered Neurons: Integrating Tree Structures into Recurrent Neural Networks</a> (ICLR 2019) [<a href="slides/20181120_jhao.pdf"><font color="#0080FF">slides</font></a>]
  </td> 
  <td> <a href="https://demon-jiehao.github.io/">Jie Hao</a> </td>
</tr>
<tr>
  <td>
    <a href="https://openreview.net/forum?id=BkltNhC9FX">Posterior Attention Models for Sequence to Sequence Learning</a> (ICLR 2019) [<a href="slides/20181120_xwgeng.pdf"><font color="#0080FF">slides</font></a>]
  </td>
  <td> <a href="http://ir.hit.edu.cn/~xwgeng/">Xinwei Geng</a> </td>
</tr>
<tr>
  <td>
    <a href="https://openreview.net/forum?id=HJz6tiCqYm">Benchmarking Neural Network Robustness to Common Corruptions and Perturbations</a> (ICLR 2019) [<a href="slides/20181120_slhe.pdf"><font color="#0080FF">slides</font></a>]
  </td>
  <td> <a href="https://shilinhe.github.io">Shilin He</a> </td>
</tr>

<tr> 
  <td rowspan=4>2018-11-13</td>
  <td>
    An Analysis of Encoder Representations in Transformer-Based Machine Translation (EMNLP 2018 Workshop BlackboxNLP) [<a href="slides/20181113_xwang.pdf"><font color="#0080FF">slides</font></a>]
  </td>
  <td> <a href="http://www.xingwang4nlp.com">Xing Wang</a> </td>
</tr>
<tr>
  <td>
    <a href="https://openreview.net/forum?id=H1z-PsR5KX">Identifying and Controlling Important Neurons in Neural Machine Translation</a> (ICLR 2019) [<a href="slides/20181113_lywang.pdf"><font color="#0080FF">slides</font></a>]
  </td>
  <td> <a href="http://www.longyuewang.com">Longyue Wang</a> </td>
</tr>
<tr>
  <td>
    Learning When to Concentrate or Divert Attention: Self-Adaptive Attention Temperature for Neural Machine Translation (EMNLP 2018) [<a href="slides/20181113_bsyang.pdf"><font color="#0080FF">slides</font></a>]
  </td> 
  <td> <a href="https://scholar.google.com/citations?hl=zh-CNBaosong Yanguser=fXsHJXkAAAAJ">Baosong Yang</a> </td>
</tr>
<tr>
  <td>
    <a href="https://openreview.net/forum?id=rygp3iRcF7">Area Attention</a> (ICLR 2019 submission) [<a href="slides/20181113_jhao.pdf"><font color="#0080FF">slides</font></a>]
  </td> 
  <td> <a href="https://demon-jiehao.github.io/">Jie Hao</a> </td>
</tr>

<tr> 
  <td rowspan=4>2018-10-23</td>
  <td>
    BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding (arXiv 2018) [<a href="slides/20181023_bsyang.pdf"><font color="#0080FF">slides</font></a>]
  </td> 
  <td> <a href="https://scholar.google.com/citations?hl=zh-CNBaosong Yanguser=fXsHJXkAAAAJ">Baosong Yang</a> </td>
</tr>
<tr>
  <td>
    <a href="https://openreview.net/forum?id=r1xN5oA5tm">Phrase-Based Attentions</a> (ICLR 2019 submission) [<a href="slides/20181023_jhao.pdf"><font color="#0080FF">slides</font></a>]
  </td> 
  <td> <a href="https://demon-jiehao.github.io/">Jie Hao</a> </td>
</tr>
<tr>
  <td>
    Beyond Error Propagation in Neural Machine Translation: Characteristics of Language Also Matter (EMNLP 2018) [<a href="slides/20181023_xwgeng.pdf"><font color="#0080FF">slides</font></a>]
  </td>
  <td> <a href="http://ir.hit.edu.cn/~xwgeng/">Xinwei Geng</a> </td>
</tr>
<tr>
  <td>
    Contextual Parameter Generation for Universal Neural Machine Translation (EMNLP 2018) <br/>
    (Self-Attentive) Autoencoder-based Universal Language Representation for Machine Translation (arXiv) [<a href="slides/20181023_yjiang.pdf"><font color="#0080FF">slides</font></a>]
  </td>
  <td> <a href="http://jiangyong.site">Yong Jiang</a> </td>
</tr>

<tr> 
  <td>2018-10-09</td>
  <td>
    <i>Capsule Networks: Models and Applications</i>, covering [<a href="slides/20181009_jli.pdf"><font color="#0080FF">slides</font></a>]<br/><br/>
    Dynamic Routing Between Capsules (NIPS 2017) <br/>
    Matrix Capsules with EM Routing (ICLR 2018) <br/>
    Information Aggregation via Dynamic Routing for Sequence Encoding (COLING 2018) <br/>
    Investigating Capsule Networks with Dynamic Routing for Text Classification (EMNLP 2018)    
  </td>
  <td> <a href="http://appsrv.cse.cuhk.edu.hk/~jianli/">Jian Li</a> </td>
</tr>

<tr> 
  <td rowspan=3>2018-09-25</td>
  <td>
    <i>Unsupervised Neural Machine Translation: A Short Survey</i> [<a href="slides/20180925_yjiang.pdf"><font color="#0080FF">slides</font></a>]
  </td> 
  <td> <a href="http://jiangyong.site">Yong Jiang</a> </td>
</tr>
<tr>
  <td>
    Has Machine Translation Achieved Human Parity? A Case for Document-level Evaluation (EMNLP 2018) <br/>
    Evaluating Syntactic Properties of Seq2Seq Output with a Broad Coverage HPSG: A Case Study on Machine Translation (arXiv 2018) <br/>
    Contextual Encoding for Translation Quality Estimation (WMT 2018) [<a href="slides/20180925_lywang.pdf"><font color="#0080FF">slides</font></a>]
  </td> 
  <td> <a href="http://www.longyuewang.com">Longyue Wang</a> </td>
</tr>
<tr>
  <td>
    Linguistically-Informed Self-Attention for Semantic Role Labeling (EMNLP 2018, <b>Best Paper</b>) [<a href="slides/20180925_jhao.pdf"><font color="#0080FF">slides</font></a>]
  </td> 
  <td> <a href="https://demon-jiehao.github.io/">Jie Hao</a> </td>
</tr>


<tr> 
  <td rowspan=5>2018-09-18</td>
  <td>
    Unsupervised Sentence Compression using Denoising Auto-Encoders (EMNLP 2018) <br/>
    Unsupervised Statistical Machine Translation (EMNLP 2018) [<a href="slides/20180918_yjiang.pdf"><font color="#0080FF">slides</font></a>]
  </td> 
  <td> <a href="http://jiangyong.site">Yong Jiang</a> </td>
</tr>
<tr>
  <td>
    How Much Attention Do You Need? A Granular Analysis of Neural Machine Translation Architectures (ACL 2018) <br/>
    The Importance of Being Recurrent for Modeling Hierarchical Structure (EMNLP 2018) [<a href="slides/20180918_jhao.pdf"><font color="#0080FF">slides</font></a>]
  </td> 
  <td> <a href="https://demon-jiehao.github.io/">Jie Hao</a> </td>
</tr>
<tr>
  <td>
    Why Self-Attention? A Targeted Evaluation of Neural Machine Translation Architectures (EMNLP 2018) [<a href="slides/20180918_bsyang.pdf"><font color="#0080FF">slides</font></a>]
  </td> 
  <td> <a href="https://scholar.google.com/citations?hl=zh-CNBaosong Yanguser=fXsHJXkAAAAJ">Baosong Yang</a> </td>
</tr>
<tr>
  <td>
    On The Alignment Problem In Multi-Head Attention-Based Neural Machine Translation (WMT 2018) [<a href="slides/20180918_jli.pdf"><font color="#0080FF">slides</font></a>]
  </td>
  <td> <a href="http://appsrv.cse.cuhk.edu.hk/~jianli/">Jian Li</a> </td>
</tr>
<tr>
  <td>
    Speeding Up Neural Machine Translation Decoding by Cube Pruning (EMNLP 2018) <br/>
    Breaking the Beam Search Curse: A Study of (Re-)Scoring Methods and Stopping Criteria for Neural Machine Translation (EMNLP 2018) [<a href="slides/20180918_xwgeng.pdf"><font color="#0080FF">slides</font></a>]
  </td>
  <td> <a href="http://ir.hit.edu.cn/~xwgeng/">Xinwei Geng</a> </td>
</tr>

<tr> 
  <td rowspan=5>2018-09-11</td>
  <td>
    A Brief Survey on Unsupervised Neural&Statistical Machine Translation
  </td> 
  <td> <a href="http://www.xingwang4nlp.com">Xing Wang</a> </td>
</tr>
<tr>
  <td>
    Review Unsupervised NMT from a Different Perspective
  </td> 
  <td> <a href="http://jiangyong.site">Yong Jiang</a> </td>
</tr>
<tr>
  <td>
    Multi-Level Structured Self-Attentions for Distantly Supervised Relation Extraction (EMNLP 2018) <br/>
    Parameter Sharing Methods for Multilingual Self-Attentional Translation Models (WMT 2018)
  </td> 
  <td> <a href="https://scholar.google.com/citations?hl=zh-CNBaosong Yanguser=fXsHJXkAAAAJ">Baosong Yang</a> </td>
</tr>
<tr>
  <td>
    A Novel Neural Sequence Model with Multiple Attentions for Word Sense Disambiguation (ICMLA 2018)
  </td>
  <td> <a href="http://appsrv.cse.cuhk.edu.hk/~jianli/">Jian Li</a> </td>
</tr>
<tr>
  <td>
    Future-Prediction-Based Model for Neural Machine Translation (arXiv 2018)
  </td>
  <td> <a href="http://ir.hit.edu.cn/~xwgeng/">Xinwei Geng</a> </td>
</tr>

<tr> 
  <td rowspan=3>2018-08-06</td>
  <td>
    Latent Alignment and Variational Attention (arXiv 2018) <br/>
    Auto-Encoding Variational Neural Machine Translation (arXiv 2018) [<a href="slides/20180806_yjiang.pdf"><font color="#0080FF">slides</font></a>]
  </td> 
  <td> <a href="http://jiangyong.site">Yong Jiang</a> </td>
</tr>
<tr>
  <td>
    Phrase-Based & Neural Unsupervised Machine Translation (EMNLP 2018, <b>Best Paper</b>) [<a href="slides/20180806_bsyang.pdf"><font color="#0080FF">slides</font></a>]
  </td> 
  <td> <a href="https://scholar.google.com/citations?hl=zh-CNBaosong Yanguser=fXsHJXkAAAAJ">Baosong Yang</a> </td>
</tr>
<tr>
  <td>
    Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding (arXiv 2018) [<a href="slides/20180806_jli.pdf"><font color="#0080FF">slides</font></a>]
  </td>
  <td> <a href="http://appsrv.cse.cuhk.edu.hk/~jianli/">Jian Li</a> </td>
</tr>

<tr> 
  <td rowspan=1>2018-07-30</td>
  <td>
    <i>Survey on Pre-Trained Word Embedding</i>, covering [<a href="slides/20180730_xwang.pdf"><font color="#0080FF">slides</font></a>]<br/>
    When and Why are Pre-trained Word Embeddings Useful for Neural Machine Translation? (arXiv 2018)<br/>
    How Do Source-side Monolingual Word Embeddings Impact Neural Machine Translation? (arXiv 2018)<br/>
    Context-Attentive Embeddings for Improved Sentence Representations (arXiv 2018)
  </td> 
  <td> <a href="http://www.xingwang4nlp.com">Xing Wang</a> </td>
</tr>

<tr> 
  <td rowspan=4>2018-07-23</td>
  <td>
    Universal Transformers (arXiv 2018) [<a href="slides/20180723_bsyang.pdf"><font color="#0080FF">slides</font></a>]
  </td> 
  <td> <a href="https://scholar.google.com/citations?hl=zh-CNBaosong Yanguser=fXsHJXkAAAAJ">Baosong Yang</a> </td>
</tr>
<tr>
  <td>
    Decoding as Continuous Optimization in Neural Machine Translation (EMNLP 2017) [<a href="slides/20180723_jli.pdf"><font color="#0080FF">slides</font></a>]
  </td>
  <td> <a href="http://appsrv.cse.cuhk.edu.hk/~jianli/">Jian Li</a> </td>
</tr>
<tr>
  <td>
    Trainable Greedy Decoding for Neural Machine Translation (EMNLP 2017) [<a href="slides/20180723_xwgeng.pdf"><font color="#0080FF">slides</font></a>]
  </td>
  <td> <a href="http://ir.hit.edu.cn/~xwgeng/">Xinwei Geng</a> </td>
</tr>
<tr>
  <td>
    Neural Machine Translation with Gumbel-Greedy Decoding (AAAI 2018) [<a href="slides/20180723_jhao.pdf"><font color="#0080FF">slides</font></a>]
  </td> 
  <td> <a href="https://demon-jiehao.github.io/">Jie Hao</a> </td>
</tr>

<tr> 
  <td rowspan=4>2018-07-09</td>
  <td>
    cw2vec - Learning Chinese Word Embeddings with Stroke n-gram Information (AAAI 2018) <br/>
    Fine-Grained Attention Mechanism for Neural Machine Translation [<a href="slides/20180709_bsyang.pdf"><font color="#0080FF">slides</font></a>]
  </td> 
  <td> <a href="https://scholar.google.com/citations?hl=zh-CNBaosong Yanguser=fXsHJXkAAAAJ">Baosong Yang</a> </td>
</tr>
<tr>
  <td>
    End-to-End Dense Video Captioning with Masked Transformer (CVPR 2018) [<a href="slides/20180709_jli.pdf"><font color="#0080FF">slides</font></a>]
  </td>
  <td> <a href="http://appsrv.cse.cuhk.edu.hk/~jianli/">Jian Li</a> </td>
</tr>
<tr>
  <td>
    The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation (ACL 2018) [<a href="slides/20180709_jhao.pdf"><font color="#0080FF">slides</font></a>]
  </td>
  <td> <a href="https://demon-jiehao.github.io/">Jie Hao</a> </td>
</tr>
<tr>
  <td>
    Reinforced Self-Attention Network: a Hybrid of Hard and Soft Attention for Sequence Modeling (IJCAI 2018) [<a href="slides/20180709_xwgeng.pdf"><font color="#0080FF">slides</font></a>]
  </td> 
  <td> <a href="http://ir.hit.edu.cn/~xwgeng/">Xinwei Geng</a> </td>
</tr>

<tr> 
  <td rowspan=5>2018-07-02</td>
  <td>
    Response Generation by Context-Aware Prototype Editing [<a href="slides/20180702_yjiang.pdf"><font color="#0080FF">slides</font></a>]
  </td> 
  <td>  <a href="http://jiangyong.site">Yong Jiang</a> </td>
</tr>
<tr>
  <td>
    <i>Sentence representation</i>, covering: [<a href="slides/20180702_xwang.pdf"><font color="#0080FF">slides</font></a>] <br/>
    <a href="https://media.nips.cc/nipsbooks/nipspapers/paper_files/nips28/reviews/1826.html">Skip-Thought Vectors</a> (NIPS 2015) <br/>
    <a href="https://openreview.net/pdf?id=rJvJXZb0W">An Efficient Framework for Learning Sentence Representations</a> (ICLR 2018)
  </td> 
  <td> <a href="http://www.xingwang4nlp.com">Xing Wang</a> </td>
</tr>
<tr>
  <td>
    Bag-of-Words as Target for Neural Machine Translation (ACL 2018) [<a href="slides/20180702_jhao.pdf"><font color="#0080FF">slides</font></a>]
  </td>
  <td> <a href="https://demon-jiehao.github.io/">Jie Hao</a> </td>
</tr>
<tr>
  <td>
    Focused Hierarchical RNNs for Conditional Sequence Processing (ICML 2018) [<a href="slides/20180702_xwgeng.pdf"><font color="#0080FF">slides</font></a>]
  </td> 
  <td> <a href="http://ir.hit.edu.cn/~xwgeng/">Xinwei Geng</a> </td>
</tr>
<tr>
  <td>
    <a href="https://openreview.net/forum?id=B1ckMDqlg">Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer</a> (ICLR 2017) [<a href="slides/20180702_jli.pdf"><font color="#0080FF">slides</font></a>]
  </td>
  <td> <a href="http://appsrv.cse.cuhk.edu.hk/~jianli/">Jian Li</a> </td>
</tr>

<tr> 
  <td rowspan=5>2018-06-25</td>
  <td>
    Gated Self-Matching Networks for Reading Comprehension and Question Answering (ACL 2017) [<a href="slides/20180625_bsyang.pdf"><font color="#0080FF">slides</font></a>]
  </td> 
  <td> <a href="https://scholar.google.com/citations?hl=zh-CNBaosong Yanguser=fXsHJXkAAAAJ">Baosong Yang</a> </td>
</tr>
<tr>
  <td>
    Ruminating Reader: Reasoning with Gated Multi-Hop Attention [<a href="slides/20180625_jli.pdf"><font color="#0080FF">slides</font></a>]
  </td>
  <td> <a href="http://appsrv.cse.cuhk.edu.hk/~jianli/">Jian Li</a> </td>
</tr>
<tr>
  <td>
    <a href="https://openreview.net/forum?id=BJC_jUqxe">A Structured Self-Attentive Sentence Embedding</a> (ICLR 2017) [<a href="https://github.com/ExplorerFreda/Structured-Self-Attentive-Sentence-Embedding"><font color="#0080FF">code</font></a>] [<a href="slides/20180625_xwang.pdf"><font color="#0080FF">slides</font></a>]
  </td> 
  <td> <a href="http://www.xingwang4nlp.com">Xing Wang</a> </td>
</tr>
<tr>
  <td>
    Handling Homographs in Neural Machine Translation (NAACL 2018) [<a href="slides/20180625_jhao.pdf"><font color="#0080FF">slides</font></a>]
  </td>
  <td> <a href="https://demon-jiehao.github.io/">Jie Hao</a> </td>
</tr>
<tr>
  <td>
    Layer aggregation models, covering [<a href="slides/20180625_zydou.pdf"><font color="#0080FF">slides</font></a>]: <br>
    Densely Connected Convolutional Networks (CVPR 2017) <br>
    Dense Information Flow for Neural Machine Translation (NAACL 2018) <br>
    Deep Layer Aggregation (CVPR 2018) <br>
    Information Aggregation via Dynamic Routing for Sequence Encoding
  </td> 
  <td> <a href="https://scholar.google.com/citations?user=XiLNShEAAAAJ&hl=en">Ziyi Dou</a> </td>
</tr>

<tr> 
  <td rowspan=4>2018-06-19</td>
  <td>
    Context-Aware Neural Machine Translation Learns Anaphora Resolution (ACL 2018) [<a href="slides/20180619_bsyang.pdf"><font color="#0080FF">slides</font></a>]
  </td> 
  <td> <a href="https://scholar.google.com/citations?hl=zh-CNBaosong Yanguser=fXsHJXkAAAAJ">Baosong Yang</a> </td>
</tr>
<tr>
  <td>
    On the Evaluation of Semantic Phenomena in Neural Machine Translation Using Natural Language Inference (NAACL 2018) [<a href="slides/20180619_jli.pdf"><font color="#0080FF">slides</font></a>]
  </td>
  <td> <a href="http://appsrv.cse.cuhk.edu.hk/~jianli/">Jian Li</a> </td>
</tr>
<tr>
<td>
    Generative Neural Machine Translation [<a href="slides/20180619_xwang.pdf"><font color="#0080FF">slides</font></a>]
  </td> 
  <td> <a href="http://www.xingwang4nlp.com">Xing Wang</a> </td>
</tr>
<tr>
  <td>
    Deconvolution-Based Global Decoding for Neural Machine Translation (COLING 2018) [<a href="slides/20180619_jhao.pdf"><font color="#0080FF">slides</font></a>]
  </td>
  <td> <a href="https://demon-jiehao.github.io/">Jie Hao</a> </td>
</tr>
<!-- tr>
<td>
    Explicit Reasoning over End-to-End Neural Architectures for Visual Question Answering (AAAI 2018)
  </td> 
  <td> <a href="http://jiangyong.site">Yong Jiang</a> </td>
</tr -->

<tr> 
  <td rowspan=3>2018-06-11</td>
  <td>
    <a href="https://openreview.net/forum?id=B14TlG-RW">QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension</a> (ICLR 2018) [<a href="slides/20180611_bsyang.pdf"><font color="#0080FF">slides</font></a>]
  </td> 
  <td> <a href="https://scholar.google.com/citations?hl=zh-CNBaosong Yanguser=fXsHJXkAAAAJ">Baosong Yang</a> </td>
</tr>
<tr>
  <td>
    Multi-Cast Attention Networks for Retrieval-Based Question Answering and Response Prediction (KDD 2018) [<a href="slides/20180611_jli.pdf"><font color="#0080FF">slides</font></a>]
  </td>
  <td> <a href="http://appsrv.cse.cuhk.edu.hk/~jianli/">Jian Li</a> </td>
</tr>
<tr>
  <td>
    <a href="https://media.nips.cc/nipsbooks/nipspapers/paper_files/nips29/reviews/1235.html">Review Networks for Caption Generation</a> (NIPS 2016) [<a href="slides/20180611_jhao.pdf"><font color="#0080FF">slides</font></a>]
  </td>
  <td> <a href="https://demon-jiehao.github.io/">Jie Hao</a> </td>
</tr>

<tr> 
  <td rowspan=3>2018-06-04 <br><br> (<b>IR + Generation</b>)<br> -- NLP Center</td>
  <td>
    Lattice Based Translation Memory for Neural Machine Translation [<a href="slides/20180604_mzxia.pdf"><font color="#0080FF">slides</font></a>]
  </td> 
  <td> Mengzhou Xia </td>  
</tr>
<tr>
  <td>
    StockQA via VAE with Retrieved Information [<a href="slides/20180604_yjiang.pdf"><font color="#0080FF">slides</font></a>]
  </td> 
  <td> <a href="http://jiangyong.site">Yong Jiang</a> </td>
  </tr>
<tr>
  <td>
    Informative Neural Response Generation with Retrieval Memories [<a href="slides/20180604_dcai.pdf"><font color="#0080FF">slides</font></a>]
  </td>
  <td> <a href="https://jcyk.github.io">Deng Cai</a> </td>
</tr>

<tr> 
  <td rowspan=4>2018-04-24 <br><br> (Terminated due to EMNLP deadline,<br> moved to 2018-06-25)</td>
  <td>
    Gated Self-Matching Networks for Reading Comprehension and Question Answering (ACL 2017)
  </td> 
  <td> <a href="http://jiangyong.site">Yong Jiang</a> </td>  
</tr>
<tr>
  <td>
    <a href="https://openreview.net/forum?id=BJC_jUqxe">A Structured Self-Attentive Sentence Embedding</a> (ICLR 2017) [<a href="https://github.com/ExplorerFreda/Structured-Self-Attentive-Sentence-Embedding"><font color="#0080FF">code</font></a>]
  </td> 
  <td> <a href="https://scholar.google.com/citations?hl=zh-CNBaosong Yanguser=fXsHJXkAAAAJ">Baosong Yang</a> </td>
</tr>
<tr>
  <td>
    Ruminating Reader: Reasoning with Gated Multi-Hop Attention
  </td>
  <td> <a href="http://appsrv.cse.cuhk.edu.hk/~jianli/">Jian Li</a> </td>
</tr>
<tr>
  <td>
    Densely Connected Convolutional Networks (CVPR 2017) </br>
    Deep Layer Aggregation (CVPR 2018)
  </td> 
  <td> <a href="https://scholar.google.com/citations?user=XiLNShEAAAAJ&hl=en">Ziyi Dou</a> </td>
</tr>

<tr> 
  <td rowspan=4>2018-04-16</td>
  <td>
    Semi-Supervised Learning for Neural Machine Translation (ACL 2016) </br>
    Joint Training for Neural Machine Translation Models with Monolingual Data (AAAI 2018) [<a href="slides/20180416_yjiang.pdf"><font color="#0080FF">slides</font></a>]
  </td> 
  <td> <a href="http://jiangyong.site">Yong Jiang</a> </td>  
</tr>
<tr>
  <td>
    <a href="https://openreview.net/forum?id=H1cWzoxA-">Bi-Directional Block Self-Attention for Fast and Memory-Efficient Sequence Modeling</a> (ICLR 2018) [<a href="slides/20180416_bsyang.pdf"><font color="#0080FF">slides</font></a>]
  </td> 
  <td> <a href="https://scholar.google.com/citations?hl=zh-CNBaosong Yanguser=fXsHJXkAAAAJ">Baosong Yang</a> </td>
</tr>
<tr>
  <td>
    Distance-Based Self-Attention Network for Natural Language Inference [<a href="slides/20180416_jli.pdf"><font color="#0080FF">slides</font></a>]
  </td>
  <td> <a href="http://appsrv.cse.cuhk.edu.hk/~jianli/">Jian Li</a> </td>
</tr>
<tr>
  <td>
    Self-Attentional Acoustic Models [<a href="slides/20180416_zydou.pdf"><font color="#0080FF">slides</font></a>]
  </td> 
  <td> <a href="https://scholar.google.com/citations?user=XiLNShEAAAAJ&hl=en">Ziyi Dou</a> </td>
</tr>

<tr> 
  <td rowspan=3>2018-04-09 </br></br> (<b>Representation Learning</b>)</br> -- NLP Center</td>
  <td>
    Tutorial: <i>Neural Word Embeddings from Scratch</i> [<a href="slides/20180409_xli.pdf"><font color="#0080FF">slides</font></a>]
  </td> 
  <td> <a href="https://lixin4ever.github.io">Xin Li</a> </td>  
</tr>
<tr>
  <td>
    <i>Contextual Representation Learning</i> [<a href="slides/20180409_zydou.pdf"><font color="#0080FF">slides</font></a>], covering </br></br>
    Learned in Translation: Contextualized Word Vectors (NIPS 2017) </br>
    Deep Contextualized Word Representations (NAACL 2018) </br>
    Universal Sentence Encoder
  </td> 
  <td> <a href="https://scholar.google.com/citations?user=XiLNShEAAAAJ&hl=en">Ziyi Dou</a> </td>
</tr>
<tr>
  <td>
    <i>Task-Specific Representation Learning</i> [<a href="slides/20180409_dcai.pdf"><font color="#0080FF">slides</font></a>], covering </br></br>
    LightRNN: Memory and Computation-Efficient Recurrent Neural Networks (NIPS 2016) </br>
    Dependency Based Word Embedding (ACL 2014)
  </td>
  <td> <a href="https://jcyk.github.io">Deng Cai</a> </td>
</tr>

<tr> 
  <td rowspan=4>2018-04-08</td>
  <td>
    Attention on Attention: Architectures for Visual Question Answering [<a href="slides/20180408_yjiang.pdf"><font color="#0080FF">slides</font></a>]
  </td> 
  <td> <a href="http://jiangyong.site">Yong Jiang</a> </td>  
</tr>
<tr>
  <td>
    The Importance of Being Recurrent for Modeling Hierarchical Structure </br>
    Colorless Green Recurrent Networks Dream Hierarchically (NAACL 2018) [<a href="slides/20180408_bsyang.pdf"><font color="#0080FF">slides</font></a>]
  </td> 
  <td> <a href="https://scholar.google.com/citations?hl=zh-CNBaosong Yanguser=fXsHJXkAAAAJ">Baosong Yang</a> </td>
</tr>
<tr>
  <td>
    Identifying Semantic Divergences in Parallel Text without Annotations [<a href="slides/20180408_jli.pdf"><font color="#0080FF">slides</font></a>]
  </td>
  <td> <a href="http://appsrv.cse.cuhk.edu.hk/~jianli/">Jian Li</a> </td>
</tr>
<tr>
  <td>
    Universal Sentence Encoder [<a href="slides/20180408_zydou.pdf"><font color="#0080FF">slides</font></a>]
  </td> 
  <td> <a href="https://scholar.google.com/citations?user=XiLNShEAAAAJ&hl=en">Ziyi Dou</a> </td>
</tr>

<tr> 
  <td rowspan=4>2018-04-02</td>
  <td>
    Generating Sentences by Editing Prototypes </br>
    <a href="https://openreview.net/forum?id=HJewuJWCZ">Learning to Teach</a> (ICLR 2018) [<a href="slides/20180402_yjiang.pdf"><font color="#0080FF">slides</font></a>]
  </td> 
  <td> <a href="http://jiangyong.site">Yong Jiang</a> </td>  
</tr>
<tr>
  <td>
    <a href="https://openreview.net/forum?id=HkyYqU9lx">Sequence to Sequence Transduction with Hard Monotonic Attention</a> (ICRL 2017, reject) </br>
    Morphological Inflection Generation with Hard Monotonic Attention (ACL 2017) [<a href="slides/20180402_bsyang.pdf"><font color="#0080FF">slides</font></a>]
  </td> 
  <td> <a href="https://scholar.google.com/citations?hl=zh-CNBaosong Yanguser=fXsHJXkAAAAJ">Baosong Yang</a> </td>
</tr>
<tr>
  <td>
    Plan, Attend, Generate: Planning for Sequence-to-Sequence Models (NIPS 2017) [<a href="slides/20180402_jli.pdf"><font color="#0080FF">slides</font></a>]
  </td> 
  <td> <a href="http://appsrv.cse.cuhk.edu.hk/~jianli/">Jian Li</a> </td>
</tr>
<tr>
  <td>
    Learned in Translation: Contextualized Word Vectors (NIPS 2017) </br>
    Deep Contextualized Word Representations (NAACL 2018) [<a href="slides/20180402_zydou.pdf"><font color="#0080FF">slides</font></a>]
  </td> 
  <td> <a href="https://scholar.google.com/citations?user=XiLNShEAAAAJ&hl=en">Ziyi Dou</a> </td>
</tr>

<tr>
  <td rowspan=3>2018-03-26 </br></br> (<b>RL</b>) -- NLP Center</td> 
  <td>
     <i>A Tutorial on Reinforcement Learning</i> [<a href="slides/20180326_rl.pdf"><font color="#0080FF">slides</font></a>]
  </td> 
  <td> Zhi Wang </td>
</tr>
<tr>
  <td>
    An Actor-Critic Algorithm for Sequence Prediction (ICLR 2017) [<a href="slides/20180326_ac.pdf"><font color="#0080FF">slides</font></a>]
  </td> 
  <td> <a href="http://www.ee.cuhk.edu.hk/~xtli/">Xintong Li</a> </td>
<tr> 
  <td>
    Decoding with Value Networks for Neural Machine Translation  [<a href="slides/20180326_value.pdf"><font color="#0080FF">slides</font></a>]
  </td> 
  <td> <a href="https://github.com/Epsilon-Lee">Guanlin Li</a> </td>
</tr>

<tr> 
  <td rowspan=3>2018-03-19</td>
  <td>
    DiSAN: Directional Self-Attention Network for RNN/CNN-Free Language Understanding (AAAI 2018) </br>
    Self-Attention with Relative Position Representations (NAACL 2018) [<a href="slides/20180319_bsyang.pdf"><font color="#0080FF">slides</font></a>]
  </td> 
  <td> <a href="https://scholar.google.com/citations?hl=zh-CNBaosong Yanguser=fXsHJXkAAAAJ">Baosong Yang</a> </td>
</tr>
<tr>
  <td>
    <a href="https://openreview.net/forum?id=HkwZSG-CZ">Breaking the Softmax Bottleneck: A High-Rank RNN Language Model</a> (ICLR 2018) [<a href="slides/20180319_jli.pdf"><font color="#0080FF">slides</font></a>]
  </td> 
  <td> <a href="http://appsrv.cse.cuhk.edu.hk/~jianli/">Jian Li</a> </td>
</tr>
<tr>
  <td>
    Tied Multitask Learning for Neural Speech Translation (NAACL 2018) [<a href="slides/20180319_zydou.pdf"><font color="#0080FF">slides</font></a>]
  </td> 
  <td> <a href="https://scholar.google.com/citations?user=XiLNShEAAAAJ&hl=en">Ziyi Dou</a> </td>
</tr>

<tr>
  <td rowspan=3>2018-03-12 </br></br> (<b>VAE</b>) -- NLP Center </td> 
  <td>
     <i>Variational Auto-Encoder: A Survey</i> [<a href="slides/20180312_yjiang.pdf"><font color="#0080FF">slides</font></a>] </br></br>
       A Hierarchical Latent Variable Encoder-Decoder Model for Generating Dialogues (AAAI 2017) </br>
       Latent Variable Dialogue Models and their Diversity (EACL 2017)
  </td> 
  <td> <a href="http://jiangyong.site">Yong Jiang</a> </td>
</tr>
<tr>
  <td>
    Variational Neural Machine Translation (EMNLP 2016) </br>
    Variational Recurrent Neural Machine Translation (AAAI 2018) [<a href="slides/20180312_bsyang.pdf"><font color="#0080FF">slides</font></a>]
  </td> 
  <td> <a href="https://scholar.google.com/citations?hl=zh-CNBaosong Yanguser=fXsHJXkAAAAJ">Baosong Yang</a> </td>
<tr> 
  <td>
    Variational Attention for Sequence-to-Sequence Models  [<a href="slides/20180312_jli.pdf"><font color="#0080FF">slides</font></a>]
  </td> 
  <td> <a href="http://appsrv.cse.cuhk.edu.hk/~jianli/">Jian Li</a> </td>
</tr>

<tr>
  <td rowspan=2>2018-02-05</td> 
  <td> 
    <a href="https://openreview.net/forum?id=HkE0Nvqlg">Structured Attention Networks</a> (ICLR 2017)
  </td>
  <td> <a href="https://scholar.google.com/citations?hl=zh-CNBaosong Yanguser=fXsHJXkAAAAJ">Baosong Yang</a> </td>
</tr>
<tr>
  <td>
    Incorporating Structural Alignment Biases into an Attentional Neural Translation Model (NAACL 2016)
  </td> 
  <td> <a href="http://www.xingwang4nlp.com">Xing Wang</a> </td>
</tr>

<tr>
  <td>2018-01-29</td> 
  <td> 
    <a href="https://openreview.net/forum?id=Sy2ogebAW">Unsupervised Neural Machine Translation</a> (ICLR 2018) </br> 
    <a href="https://openreview.net/forum?id=rkYTTf-AZ">Unsupervised Machine Translation Using Monolingual Corpora Only</a> (ICLR 2018) </td> 
  <td> <a href="https://scholar.google.com/citations?hl=zh-CNBaosong Yanguser=fXsHJXkAAAAJ">Baosong Yang</a> </td>
</tr>

<tr>
  <td rowspan=2>2018-01-22</td> 
  <td> 
    <a href="https://openreview.net/forum?id=B1l8BtlCb">Non-Autoregressive Neural Machine Translation</a> (ICLR 2018)
  </td> 
  <td> <a href="http://www.xingwang4nlp.com">Xing Wang</a> </td>
</tr>
<tr> 
  <td> 
    Order-Planning Neural Text Generation From Structured Data (AAAI 2018) </br> 
    Table-to-Text Generation by Structure-aware Seq2seq Learning (AAAI 2018)
  </td> 
  <td> <a href="http://jiangyong.site">Yong Jiang</a> </td>
</tr>

</table>



</body>
</html>